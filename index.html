<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Helping</title>
    <style>
        div{
            padding-top: 100vh;
            
        }

        a{
            color: aquamarine;
            text-decoration: none;
        }
    </style>
</head>
<body>
    <div>
        <a href="./public/resources/ANN.pdf">ANN sem 6</a>
        <li>
            <a href="https://drive.google.com/drive/folders/1dwxWAWVA6O-Nct4N23eAfmAQlL0NHUGo">Tej</a>
        </li>
        <a href="./tuhsar_notes.html">Tushar Notes</a>
        
<strong>HEBB</strong>
<pre>
print("\n")
def printable(x1,x2,bias,y):
    print("\t\tAND GATE logic Table")
    print("__________________________________________________________")
    print("| ","X1","\t|\t ","X2","\t|\t ","b","\t |\t","Y \t|")
    print("________________________________________________________")
    for i in range(4):
        print("| ",x1[i],"\t|\t ",x2[i],"\t|\t ",bias[i],"\t |\t",y[i],"\t|")
        print("________________________________________________________")

x1=[1,-1,1,-1]
x2=[1,1,-1,-1]
bias=[1,1,1,1]
y=[1,-1,-1,-1]

printable(x1,x2,bias,y)
w1=0
w2=0
b=0

print("\n\n")
print("\t\t\t\t HEBB LEARNING RULE TABLE")
print()
print("| ","X1","\t|\t ","X2","\t|\t ","bias","\t |\t","Y \t|","w1","\t|\t ","w2","\t|\t ","b","\t|\t ")
print("_________________________________________________________________________________________________________________")
for i in range(4):
    w1 = w1 + x1[i]*y[i]
    w2= w2+ x2[i]*y[i]

    b=b+y[i]

    print("| ",x1[i],"\t|\t ",x2[i],"\t|\t ",bias[i],"\t |\t",y[i],"\t|",w1,"\t|\t ",w2,"\t|\t ",b,"\t|\t ")
    print("__________________________________________________________________________________________________________________")


</pre>

<hr>
<br>

<strong>MCP OR</strong>
<pre>
    class MpNeuron:
  inpOuts = []

  def addinpOuput(self, inp, out):
    self.inpOuts.append([inp, out])

  def getNetinp(self, inp, weights):
    netinp = 0
    for i in range(len(inp)):
      netinp += inp[i] * weights[i]
    return netinp

  def getValidThreshold(self, weights):
    threshold = 10000000

    for inp, out in self.inpOuts:
      if out == 1:
        threshold = min(threshold, self.getNetinp(inp, weights))

    for inp, out in self.inpOuts:
      if out == 0 and threshold <= self.getNetinp(inp, weights):
        return None

    return threshold


orNeuron = MpNeuron()
orNeuron.addinpOuput([0, 0], 0)
orNeuron.addinpOuput([0, 1], 1)
orNeuron.addinpOuput([1, 0], 1)
orNeuron.addinpOuput([1, 1], 1)

print('Or Gate\'s Truth Table: ')
print('x1 x2 y')
print('-------')
print('0  0  0')
print('0  1  1')
print('1  0  1')
print('1  1  1')
print()

while True:
  weights = list(map(int, input('Enter weights: ').split(' ')))
  threshold = orNeuron.getValidThreshold(weights)

  if threshold != None:
    print('Weights are correct, Threshold Found:', threshold)
    print('Hence, and gate can be realised using mp neuron')
    break
  else:
    print('Invalid weights')
    print()

</pre>
        
<hr>
<br>

<strong>MCP XOR</strong>
<pre>

    def f(yin,theta):
    if yin >= theta: 
        return 1
    else:
        return 0

def makeGate(gateName,first,op,second,x1Header,x2Header,yHeader,yinHeader,w1,w2,theta):
    print()
    print(gateName," gate => ",yHeader," = ",first,op,second)
    
    print("We take w1 = ",w1," and w2 = ",w2)
    print()
    
    x1 = [1,1,0,0]
    x2 = [1,0,1,0]
    y = []
    
    # calculate y
    for i in range(0,4):
        if first[0] != "~" and second[0] != "~" and op == "|":
            ans = x1[i] | x2[i]
            
        elif first[0] != "~" and second[0] != "~" and op == "&":
            ans = x1[i] & x2[i]
            
        elif first[0] != "~" and second[0] == "~" and op == "|":
            ans = x1[i] | ~x2[i]
            
        elif first[0] != "~" and second[0] == "~" and op == "&":
            ans = x1[i] & ~x2[i]
            
        elif first[0] == "~" and second[0] != "~" and op == "|":
            ans = ~x1[i] | x2[i]
            
        elif first[0] == "~" and second[0] != "~" and op == "&":
            ans = ~x1[i] & x2[i]
            
        elif first[0] == "~" and second[0] == "~" and op == "|":
            ans = ~x1[i] | ~x2[i]
            
        elif first[0] == "~" and second[0] == "~" and op == "&":
            ans = ~x1[i] & ~x2[i]
            
        y.append(ans)
        
    
    yin = []
    for i in range(0,4):
        ans = w1*x1[i] + w2*x2[i]
        yin.append(ans)
    
    print(x1Header,"       ",x2Header,"       ",yHeader,"       ",yinHeader)
        
    for i in range(0,4):
        print(x1[i],"        ",x2[i],"        ",y[i],"         ",yin[i])
            
            
    print()
    print("And setting theta = ",theta)
    
    
    
    print()
    print(yHeader,"  =  f(",yinHeader,") = { ",f(theta,theta),"   ",yinHeader," >= ", theta)
    print("                   { " ,f(theta-1,theta),"   ",yinHeader," < ", theta)
    
    # line
    print()
    print("*****************************************")
    

makeGate("ANDNOT","x1","&","~x2","x1","x2","z1","z1in",1,-1,1)
makeGate("ANDNOT","~x1","&","x2","x1","x2","z2","z2in",-1,1,1)
makeGate("OR","z1","|","z2","z1","z2","y","yin",1,1,1)

print("So XOR gate can be realised by using MCP model")
</pre>
<hr>
<br>

<strong>Perceptron AND</strong>
<pre>
    import numpy as np

def sigmoid(x):
  return 1/(1+np.exp(-x))

def activation_sigmoid(x):
  y = sigmoid(x)
  if y>0:
    return 1
  elif y==0:
    return 0
  else:
    return -1

def step_function(y):
  if y>0:
    return 1
  elif y==0:
    return 0
  else:
    return -1

w = [0,0]
Wold=[0,0]
b = 0
alpha = 1

x1 = [1,1,-1,-1]
x2 = [1,-1,1,-1]
t = [1,-1,-1,-1]
epoch=True
num=1
while(epoch):
  print("\n Epoch", num ," : ")
  num=num+1
  print("_______________________________________")
  for i in range(4):
    yin = x1[i]*w[0]+x2[i]*w[1]+b
    y = step_function(yin)
    print(y)
    if(y!=t[i]):
      w[0] = w[0] + alpha*t[i]*x1[i]
      w[1] = w[1] + alpha*t[i]*x2[i]
      b = b + alpha*t[i]
      
    print(f'Weights : w1 = {w[0]} , w2 = {w[1]} , b = {b}')
  if(Wold[0]==w[0] and Wold[1]==w[1]):
      epoch=0
  Wold=w

print(f'Weights : w1 = {w[0]} , w2 = {w[1]} , b = {b}')
</pre>
<hr>
<br>

<strong>Adaline OR</strong>
<pre>
    import numpy as np

features = np.array(
    [
        [-1, -1],
        [-1, 1],
        [1, -1],
        [1, 1]
    ])

labels = np.array([-1, 1, 1, 1])

print(features, labels)

weight = [0, 0]
bias = 1
learning_rate = 0.2
epoch = 10

for i in range(epoch):

    print("epoch :", i+1)

    sum_squared_error = 0.0

    for j in range(features.shape[0]):

        actual = labels[j]

        x1 = features[j][0]
        x2 = features[j][1]

        unit = (x1 * weight[0]) + (x2 * weight[1]) + bias

        error = actual - unit

        print("error =", error)

        sum_squared_error += error * error

        weight[0] += learning_rate * error * x1
        weight[1] += learning_rate * error * x2

        bias += learning_rate * error

    print("sum of squared error = ", sum_squared_error/4, "\n\n")

</pre>
<hr>
<br>

<strong>Adaline AND</strong>
<pre>
    import numpy as np

features = np.array(
    [
        [-1, -1],
        [-1, 1],
        [1, -1],
        [1, 1]
    ])

labels = np.array([-1, -1, -1, 1])

print(features, labels)

weight = [0, 0]
bias = 1
learning_rate = 0.2
epoch = 10

for i in range(epoch):

    print("epoch :", i+1)

    sum_squared_error = 0.0

    for j in range(features.shape[0]):

        actual = labels[j]

        x1 = features[j][0]
        x2 = features[j][1]

        unit = (x1 * weight[0]) + (x2 * weight[1]) + bias

        error = actual - unit

        print("error =", error)

        sum_squared_error += error * error

        weight[0] += learning_rate * error * x1
        weight[1] += learning_rate * error * x2

        bias += learning_rate * error

    print("sum of squared error = ", sum_squared_error/4, "\n\n")

</pre>
<hr>
<br>


<strong>Perceptron ANDNOT</strong>
<pre>
    def activationFn(x, theta):
    if x < -theta:
      return -1
    if -theta <= x <= theta:
      return 0
    return 1
  
  
  class Perceptron:
    def __init__(self, N, theta=0, alpha=1):
      self.N = N
      self.weights = [0] * N
      self.bias = 0
      self.theta = theta
      self.alpha = alpha
      self.expectedInOuts = []
  
    def addexpectedInOut(self, inp, out):
      self.expectedInOuts.append([inp, out])
  
    def getNetinp(self, inp):
      netinp = self.bias
      for i in range(len(inp)):
        netinp += inp[i] * self.weights[i]
      return netinp
  
    def getNetOut(self, inp):
      return activationFn(self.getNetinp(inp), self.theta)
  
    # eg - x1 x2 1  t  yin  y 	dw1 dw2 db	w1 w2 b
    def printLine(self, x, t, yin, y, dw, db, w, b):
      def formattedStr(str): return '{:>4}'.format(str)
  
      s = ''
      for val in x:
        s += formattedStr(val)
      s += formattedStr(1)
      s += '  |  '
      s += formattedStr(t)
      s += '  |  '
      s += formattedStr(yin)
      s += '  |  '
      s += formattedStr(y)
      s += '  |  '
      for val in dw:
        s += formattedStr(val)
      s += formattedStr(db)
      s += '  |  '
      for val in w:
        s += formattedStr(val)
      s += formattedStr(b)
  
      print(s)
  
    def train(self):
      changed = True
      epoch = 1
  
      self.printLine(
          ['x{}'.format(i) for i in range(1, self.N + 1)],
          't',
          'yin',
          'y',
          ['dw{}'.format(i) for i in range(1, self.N + 1)],
          'db',
          ['w{}'.format(i) for i in range(1, self.N + 1)],
          'b'
      )
  
      while changed:
        changed = False
        print('EPOCH -', epoch)
  
        for x, t in self.expectedInOuts:
          yin = self.getNetinp(x)
          y = self.getNetOut(x)
          dw = [0] * self.N
          db = 0
          if y != t:
            for i in range(self.N):
              dw[i] = self.alpha * t * x[i]
              self.weights[i] += dw[i]
            db = self.alpha * t
            self.bias += db
            changed = True
  
          self.printLine(x, t, yin, y, dw, db, self.weights, self.bias)
  
        epoch += 1
  
    def test(self, inp):
      return self.getNetOut(inp)
  
  
  andNot = Perceptron(2, theta=0, alpha=1)
  andNot.addexpectedInOut([1, 1], -1)
  andNot.addexpectedInOut([1, -1], 1)
  andNot.addexpectedInOut([-1, 1], -1)
  andNot.addexpectedInOut([-1, -1], -1)
  
  andNot.train()
  print(andNot.weights, andNot.bias)
  
</pre>
<hr>
<br>

<strong>Auto Associative</strong>
<pre>
    from numpy import size

#activation function
def f(yinj):
    if yinj > 0:
        return 1
    else:
        return -1

print("Auto Associative Networks::")

t = int(input("Enter number of input samples: "))
n = int(input("Enter the number of nodes: "))

X = []
for i in range(t):          # A for loop for row entries
    a = list(map(int, input().split()))
    X.append(a)

Y = X

print("Input Vector is")
for i in range(len(X)):
    print(X[i])
print("\n____________________________\n")
print("Output Vector is")
for i in range(len(Y)):
    print(Y[i])

weights = [[0 for _ in range(n)] for _ in range(n)]

# Training Phase
for k in range(t):
    for i in range(n):
        for j in range(n):
            weights[i][j] += X[k][i]*Y[k][j]


print("\n____________________________\n")
print("Weights after Training:")
for i in range(len(weights)):
    print(weights[i])


while(1):
    print("Enter the testing vector: ")
    # Testing Phase
    test = list(map(int, input().split()))

    print("Test Input", test)

    outs = []
    for j in range(n):
        yinj = 0
        for i in range(n):
            yinj += test[i]*weights[i][j]
        yin = f(yinj)
        outs.append(yin)

    print("Testing Output", outs)
    print("\n")


    AUTO ASSOICATVE 
    2
    4
    1 1 -1 1
    1 -1 -1 -1
</pre>


<hr>
<br>

<strong>Heteroassociative</strong>
<pre>
    print("Heteroassociative Networks::")


def f(yinj):
    if yinj > 0:
        return 1
    if yinj == 0:
        return 0
    else:
        return -1


t = int(input("Enter number of input samples: "))
n = int(input("Enter the number of features: "))
n2 = int(input("Enter length of output: "))
X = []
Y = []
for i in range(t):          # A for loop for row entries
    print("input")
    a = list(map(int, input().split()))
    print("output")
    b = list(map(int, input().split()))
    X.append(a)
    Y.append(b)


print("Input Vector is")
for i in range(len(X)):
    print(X[i])
print("\n____________________________\n")
print("Output Vector is")
for i in range(len(Y)):
    print(Y[i])

weights = [[0 for _ in range(n2)] for _ in range(n)]

# Training Phase
for k in range(t):
    for i in range(n):
        for j in range(n2):
            weights[i][j] += X[k][i]*Y[k][j]

print("\n____________________________\n")
print("Weights after Training:")
for i in range(len(weights)):
    print(weights[i])



# Testing Phase
num = int(input("Enter number of test cases: "))
for p in range(num):
    print("Enter the testing vector: ")

    test = list(map(int, input().split()))
    print("Test Input", test)
    outs = []
    for j in range(n2):
        yinj = 0
        for i in range(n):
            yinj += test[i]*weights[i][j]
        yin = f(yinj)
        outs.append(yin)

    print("Testing Output", outs)
    
    
    HETERO ASSOCIATIVE 
    2
    4
    2
    1 1 1 1
    1 -1
    1 -1 1 -1
    -1 1
</pre>

<hr>
<br>

<strong>BAM</strong>
<pre>
    # Import Python Libraries
import numpy as np

# Take two sets of patterns:
# Set A: Input Pattern
# make 6*1 matrix of inputs
x1 = np.array([1, -1,-1,-1]).reshape(1, 4)
x2 = np.array([1,-1,-1,1]).reshape(1, 4)
x3 = np.array([-1,1,-1,-1]).reshape(1, 4)
x4 = np.array([-1,1,1,-1]).reshape(1, 4)

# Set B: Target Pattern
y1 = np.array([1,-1]).reshape(1,2)
y2 = np.array([1,-1]).reshape(1,2)
y3 = np.array([-1, 1]).reshape(1,2)
y4 = np.array([-1, 1]).reshape(1,2)


print("Set A: Input Pattern, Set B: Target Pattern")
print("\nThe input for pattern 1 is")
print(x1)
print("\nThe target for pattern 1 is")
print(y1)
print("\nThe input for pattern 2 is")
print(x2)
print("\nThe target for pattern 2 is")
print(y2)
print("\nThe input for pattern 3 is")
print(x3)
print("\nThe target for pattern 3 is")
print(y3)
print("\nThe input for pattern 4 is")
print(x4)
print("\nThe target for pattern 4 is")
print(y4)

print("\n------------------------------")

# Calculate weight Matrix: W
inputSet = np.concatenate((x1, x2, x3, x4), axis=0)
targetSet = np.concatenate((y1, y2, y3, y4), axis=0)
print("\nWeight matrix:")
weight = np.dot(inputSet.T, targetSet)
print(weight)

print("\n------------------------------")

# Testing Phase
# Test for Input Patterns: Set A
print("\nTesting for input patterns: Set A")


def testInputs(x, weight):

    # Multiply the input pattern with the weight matrix
    # (weight.T X x)
    y = np.dot(x, weight)
    y[y < 0] = -1
    y[y >= 0] = 1
    return np.array(y)


print("\nOutput of input pattern 1")
print(testInputs(x1, weight))
print("\nOutput of input pattern 2")
print(testInputs(x2, weight))
print("\nOutput of input pattern 3")
print(testInputs(x3, weight))
print("\nOutput of input pattern 4")
print(testInputs(x4, weight))

# Test for Target Patterns: Set B
print("\nTesting for target patterns: Set B")


def testTargets(y, weight):

    # Multiply the target pattern with the weight matrix
    # (weight X y)
    x = np.dot(y, weight.T)
    x[x <= 0] = -1
    x[x > 0] = 1
    return np.array(x)


print("\nOutput of target pattern 1")
print(testTargets(y1, weight))
print("\nOutput of target pattern 2")
print(testTargets(y2, weight))
print("\nOutput of target pattern 3")
print(testTargets(y3, weight))
print("\nOutput of target pattern 4")
print(testTargets(y4, weight))

</pre>
<hr>
<br>

<strong>Hopiefied(Tej)</strong>
<pre>
import numpy as np 
print("Input Vector: [1,1,1,-1]") 
ipvector1 = np.array([1,1,1,1,1]).reshape(1,5) 
ipvector2 = np.array([1,-1,-1,1,-1]).reshape(1,5) 
ipvector3 = np.array([-1,1,-1,-1,-1]).reshape(1,5) 

def activation(x): 
    if(x>0): return 1
    else: return 0
w1=np.transpose(ipvector1)*ipvector1
w2=np.transpose(ipvector2)*ipvector2
w3=np.transpose(ipvector3)*ipvector3

w=w1+w2+w3

for i in range(4): 
    for j in range(4): 
        if(i==j): 
            w[i][j]=0 
print("\n Weight Matrix: ")
print(w) 
#converting to binary
ipvector1 = np.array([1,1,1,1,1])
ipvector2 = np.array([1,0,0,1,0])
ipvector3 = np.array([0,1,0,0,0])
# print("Input vector in binary representation: ",ipvector) 
x=np.array([1,1,1,0,1]).reshape(1,5) 
y=x
for i in range(5): 
    yin = x[0][i] + np.dot(y,w[:,i]) 
    print("yin_",i+1,": ",yin) 
    y[0][i]=activation(yin) 
    print("Updated Y :",y) 
    if((y==ipvector1).all()): 
        print("Hence y=input vector x, vector converge") 
        break
    if((y==ipvector2).all()): 
        print("Hence y=input vector x, vector converge") 
        break
    if((y==ipvector3).all()): 
        print("Hence y=input vector x, vector converge") 
        break
</pre>
<hr>
<br>
<strong>Hopiefied (plot)</strong>
<pre>
    
import matplotlib.pyplot as plt
import numpy as np

nb_patterns = 4   # Number of patterns to learn
pattern_width = 5
pattern_height = 5
max_iterations = 10

# Define Patterns
patterns = np.array([
    [1, -1, -1, -1, 1, 1, -1, 1, 1, -1, 1, -1, 1, 1, -1,
        1, -1, 1, 1, -1, 1, -1, -1, -1, 1.],   # Letter D
    [-1, -1, -1, -1, -1, 1, 1, 1, -1, 1, 1, 1, 1, -1, 1, - \
     1, 1, 1, -1, 1, -1, -1, -1, 1, 1.],    # Letter J
    [1, -1, -1, -1, -1, -1, 1, 1, 1, 1, -1, 1, 1, 1, 1, - \
     1, 1, 1, 1, 1, 1, -1, -1, -1, -1.],     # Letter C
    [-1, 1, 1, 1, -1, -1, -1, 1, -1, -1, -1, 1, -1, 1, -1, -1, 1, 1, 1, -1, -1, 1, 1, 1, -1.], ],  # Letter M
    dtype=np.float)
fig, ax = plt.subplots(1, nb_patterns, figsize=(15, 10))

for i in range(nb_patterns):
    ax[i].matshow(patterns[i].reshape(
        (pattern_height, pattern_width)), cmap='gray')
    ax[i].set_xticks([])
    ax[i].set_yticks([])
W = np.zeros((pattern_width * pattern_height, pattern_width * pattern_height))

for i in range(pattern_width * pattern_height):
    for j in range(pattern_width * pattern_height):
        if i == j or W[i, j] != 0.0:
            continue

        w = 0.0

        for n in range(nb_patterns):
            w += patterns[n, i] * patterns[n, j]

        W[i, j] = w / patterns.shape[0]
        W[j, i] = W[i, j]
S = np.array([1, -1, -1, -1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, -1, -1, -1.],
             dtype=np.float)

# Show the corrupted pattern
fig, ax = plt.subplots()
ax.matshow(S.reshape((pattern_height, pattern_width)), cmap='gray')
h = np.zeros((pattern_width * pattern_height))
# Defining Hamming Distance matrix for seeing convergence
hamming_distance = np.zeros((max_iterations, nb_patterns))
for iteration in range(max_iterations):
    for i in range(pattern_width * pattern_height):
        i = np.random.randint(pattern_width * pattern_height)
        h[i] = 0
        for j in range(pattern_width * pattern_height):
            h[i] += W[i, j]*S[j]
        S = np.where(h < 0, -1, 1)
    for i in range(nb_patterns):
        hamming_distance[iteration, i] = ((patterns - S)[i] != 0).sum()

    fig, ax = plt.subplots()
    ax.matshow(S.reshape((pattern_height, pattern_width)), cmap='gray')
plt.show()
hamming_distance

</pre>

<hr>
<br>
<strong>Self Organising Maps</strong>
<pre>
    
import matplotlib.pyplot as plt
import numpy as np

nb_patterns = 4   # Number of patterns to learn
pattern_width = 5
pattern_height = 5
max_iterations = 10

# Define Patterns
patterns = np.array([
    [1, -1, -1, -1, 1, 1, -1, 1, 1, -1, 1, -1, 1, 1, -1,
        1, -1, 1, 1, -1, 1, -1, -1, -1, 1.],   # Letter D
    [-1, -1, -1, -1, -1, 1, 1, 1, -1, 1, 1, 1, 1, -1, 1, - \
     1, 1, 1, -1, 1, -1, -1, -1, 1, 1.],    # Letter J
    [1, -1, -1, -1, -1, -1, 1, 1, 1, 1, -1, 1, 1, 1, 1, - \
     1, 1, 1, 1, 1, 1, -1, -1, -1, -1.],     # Letter C
    [-1, 1, 1, 1, -1, -1, -1, 1, -1, -1, -1, 1, -1, 1, -1, -1, 1, 1, 1, -1, -1, 1, 1, 1, -1.], ],  # Letter M
    dtype=np.float)
fig, ax = plt.subplots(1, nb_patterns, figsize=(15, 10))

for i in range(nb_patterns):
    ax[i].matshow(patterns[i].reshape(
        (pattern_height, pattern_width)), cmap='gray')
    ax[i].set_xticks([])
    ax[i].set_yticks([])
W = np.zeros((pattern_width * pattern_height, pattern_width * pattern_height))

for i in range(pattern_width * pattern_height):
    for j in range(pattern_width * pattern_height):
        if i == j or W[i, j] != 0.0:
            continue

        w = 0.0

        for n in range(nb_patterns):
            w += patterns[n, i] * patterns[n, j]

        W[i, j] = w / patterns.shape[0]
        W[j, i] = W[i, j]
S = np.array([1, -1, -1, -1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, -1, -1, -1.],
             dtype=np.float)

# Show the corrupted pattern
fig, ax = plt.subplots()
ax.matshow(S.reshape((pattern_height, pattern_width)), cmap='gray')
h = np.zeros((pattern_width * pattern_height))
# Defining Hamming Distance matrix for seeing convergence
hamming_distance = np.zeros((max_iterations, nb_patterns))
for iteration in range(max_iterations):
    for i in range(pattern_width * pattern_height):
        i = np.random.randint(pattern_width * pattern_height)
        h[i] = 0
        for j in range(pattern_width * pattern_height):
            h[i] += W[i, j]*S[j]
        S = np.where(h < 0, -1, 1)
    for i in range(nb_patterns):
        hamming_distance[iteration, i] = ((patterns - S)[i] != 0).sum()

    fig, ax = plt.subplots()
    ax.matshow(S.reshape((pattern_height, pattern_width)), cmap='gray')
plt.show()
hamming_distance

</pre>

<hr>
<br>

<strong>SOM video</strong>
<pre>
    #Self organizing map clustering algorithm
# The Academician
import numpy as np, numpy.random
from scipy.spatial import distance
np.set_printoptions(suppress=True) #Force-suppress all exponential notation

k = 2
p = 0
alpha = 0.7 # Initial learning rate

X = np.array([
        [1,1,0,0], 
        [0,1,0,0], 
        [0,0,1,0], 
        [0,0,1,1]])


# Print the number of data and dimension 
n = len(X)
d = len(X[0])
addZeros = np.zeros((n, 1))
X = np.append(X, addZeros, axis=1)
print("The SOM algorithm: \n")
print("The training data: \n", X)
print("\nTotal number of data: ",n)
print("Total number of features: ",d)
print("Total number of Clusters: ",k)

C = np.zeros((k,d+1))

weight = np.random.rand(n,k)
print("\nThe initial weight: \n", np.round(weight,2))

for it in range(100): # Total number of iterations
    for i in range(n):
        distMin = 99999999
        for j in range(k):
            dist = np.square(distance.euclidean(weight[:,j], X[i,0:d]))
            if distMin>dist:
                distMin = dist
                jMin = j
        weight[:,jMin] = weight[:,jMin]*(1-alpha) + alpha*X[i,0:d]   
    alpha = 0.5*alpha
    
print("\nThe final weight: \n",np.round(weight,4))

for i in range(n):    
    cNumber = np.where(weight[i] == np.amax(weight[i]))
    X[i,d] = cNumber[0]
    
print("\nThe data with cluster number: \n", X)


    
</pre>
    </div>
</body>
</html>